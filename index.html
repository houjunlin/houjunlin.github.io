<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Junlin Hou - Academic Homepage</title>
  <meta name="author" content="Junlin Hou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&family=Playfair+Display:wght@400;600;700&display=swap" rel="stylesheet">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <style>
    :root {
      --primary-color: #2c3e50;
      --secondary-color: #3498db;
      --accent-color: #e74c3c;
      --light-color: #ecf0f1;
      --dark-color: #2c3e50;
      --text-color: #333;
      --light-text: #777;
      --shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
      --transition: all 0.3s ease;
    }
    
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: 'Lato', sans-serif;
      color: var(--text-color);
      line-height: 1.6;
      background-color: #f9f9f9;
    }
    
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 20px;
    }
    
    /* Header Section */
    .header {
      background: linear-gradient(135deg, var(--primary-color) 0%, var(--dark-color) 100%);
      color: white;
      padding: 40px 0;
      position: relative;
      overflow: hidden;
    }
    
    .header::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" preserveAspectRatio="none"><path d="M0,0 L100,0 L100,100 Z" fill="rgba(255,255,255,0.05)"/></svg>');
      background-size: 100% 100%;
    }
    
    .profile-container {
      display: flex;
      align-items: center;
      justify-content: space-between;
      flex-wrap: wrap;
    }
    
    .profile-text {
      flex: 1;
      min-width: 300px;
      padding-right: 30px;
    }
    
    .profile-image {
      flex: 0 0 280px;
      text-align: center;
    }
    
    .profile-image img {
      width: 220px;
      height: 220px;
      border-radius: 50%;
      object-fit: cover;
      border: 5px solid rgba(255, 255, 255, 0.2);
      box-shadow: var(--shadow);
      transition: var(--transition);
    }
    
    .profile-image img:hover {
      transform: scale(1.03);
    }
    
    h1.name {
      font-family: 'Playfair Display', serif;
      font-size: 2.8rem;
      margin-bottom: 10px;
      font-weight: 700;
    }
    
    .title {
      font-size: 1.4rem;
      margin-bottom: 20px;
      color: rgba(255, 255, 255, 0.9);
    }
    
    .contact-info {
      margin: 20px 0;
    }
    
    .contact-info a {
      color: rgba(255, 255, 255, 0.8);
      text-decoration: none;
      transition: var(--transition);
    }
    
    .contact-info a:hover {
      color: white;
    }
    
    .social-links {
      display: flex;
      gap: 15px;
      margin-top: 20px;
    }
    
    .social-links a {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      width: 40px;
      height: 40px;
      border-radius: 50%;
      background: rgba(255, 255, 255, 0.1);
      color: white;
      transition: var(--transition);
      text-decoration: none;
    }
    
    .social-links a:hover {
      background: var(--secondary-color);
      transform: translateY(-3px);
    }
    
    /* Content Sections */
    .section {
      background: white;
      border-radius: 10px;
      padding: 30px;
      margin: 30px 0;
      box-shadow: var(--shadow);
      transition: var(--transition);
    }
    
    .section:hover {
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
    }
    
    .section-title {
      font-family: 'Playfair Display', serif;
      font-size: 1.8rem;
      color: var(--primary-color);
      margin-bottom: 20px;
      padding-bottom: 10px;
      border-bottom: 2px solid var(--secondary-color);
      display: inline-block;
    }
    
    /* News Section */
    .news-container {
      max-height: 400px;
      overflow-y: auto;
      padding-right: 15px;
    }
    
    .news-item {
      margin-bottom: 20px;
      padding-bottom: 20px;
      border-bottom: 1px solid #eee;
      position: relative;
    }
    
    .news-item:last-child {
      border-bottom: none;
    }
    
    .news-date {
      font-weight: 700;
      color: var(--secondary-color);
      margin-bottom: 5px;
    }
    
    .news-content {
      margin-left: 0;
    }
    
    .news-content a {
      color: var(--primary-color);
      text-decoration: none;
      transition: var(--transition);
    }
    
    .news-content a:hover {
      color: var(--secondary-color);
    }
    
    /* Research Items */
    .research-item {
      display: flex;
      margin-bottom: 30px;
      padding-bottom: 30px;
      border-bottom: 1px solid #eee;
    }
    
    .research-item:last-child {
      border-bottom: none;
    }
    
    .research-image {
      flex: 0 0 180px;
      margin-right: 25px;
    }
    
    .research-image img {
      width: 100%;
      height: 120px;
      object-fit: cover;
      border-radius: 8px;
      transition: var(--transition);
    }
    
    .research-image img:hover {
      transform: scale(1.05);
    }
    
    .research-details {
      flex: 1;
    }
    
    .research-title {
      font-weight: 700;
      font-size: 1.2rem;
      margin-bottom: 8px;
    }
    
    .research-title a {
      color: var(--dark-color);
      text-decoration: none;
      transition: var(--transition);
    }
    
    .research-title a:hover {
      color: var(--secondary-color);
    }
    
    .research-authors {
      color: var(--light-text);
      margin-bottom: 8px;
      font-style: italic;
    }
    
    .research-conference {
      color: var(--secondary-color);
      font-weight: 700;
      margin-bottom: 10px;
    }
    
    .research-links {
      display: flex;
      gap: 15px;
    }
    
    .research-links a {
      display: inline-block;
      padding: 5px 12px;
      background: var(--light-color);
      border-radius: 4px;
      color: var(--dark-color);
      text-decoration: none;
      font-size: 0.9rem;
      transition: var(--transition);
    }
    
    .research-links a:hover {
      background: var(--secondary-color);
      color: white;
    }
    
    /* Lists */
    .custom-list {
      list-style-type: none;
    }
    
    .custom-list li {
      margin-bottom: 12px;
      padding-left: 25px;
      position: relative;
    }
    
    .custom-list li:before {
      content: "•";
      color: var(--secondary-color);
      font-weight: bold;
      display: inline-block;
      width: 1em;
      margin-left: -1em;
      font-size: 1.2rem;
    }
    
    /* Back to top button */
    .back-to-top {
      position: fixed;
      bottom: 30px;
      right: 30px;
      width: 50px;
      height: 50px;
      border-radius: 50%;
      background: var(--secondary-color);
      color: white;
      display: flex;
      align-items: center;
      justify-content: center;
      text-decoration: none;
      box-shadow: var(--shadow);
      transition: var(--transition);
      opacity: 0;
      visibility: hidden;
      z-index: 1000;
    }
    
    .back-to-top.visible {
      opacity: 1;
      visibility: visible;
    }
    
    .back-to-top:hover {
      background: var(--primary-color);
      transform: translateY(-5px);
    }
    
    /* Footer */
    .footer {
      text-align: center;
      padding: 30px 0;
      color: var(--light-text);
      font-size: 0.9rem;
      margin-top: 50px;
      border-top: 1px solid #eee;
    }
    
    /* Responsive Design */
    @media (max-width: 768px) {
      .profile-container {
        flex-direction: column-reverse;
      }
      
      .profile-text {
        padding-right: 0;
        text-align: center;
        margin-top: 30px;
      }
      
      .social-links {
        justify-content: center;
      }
      
      .research-item {
        flex-direction: column;
      }
      
      .research-image {
        margin-right: 0;
        margin-bottom: 15px;
        flex: 0 0 auto;
      }
      
      h1.name {
        font-size: 2.2rem;
      }
    }
  </style>
</head>

<body>
  <!-- Header Section -->
  <header class="header">
    <div class="container profile-container">
      <div class="profile-text">
        <h1 class="name">Junlin Hou</h1>
        <div class="title">Post-doc Fellow at HKUST</div>
        <div class="contact-info">
          <a href="mailto:csejlhou@ust.hk"><i class="fas fa-envelope"></i> csejlhou@ust.hk</a>
        </div>
        <p>My research interest is in explainable artificial intelligence for medical imaging, at an intersection of machine learning, computer vision, and medical imaging.</p>
        <div class="social-links">
          <a href="https://scholar.google.com/citations?user=2bHYfQcAAAAJ&hl=en&oi=ao" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
          <a href="https://github.com/houjunlin" title="GitHub"><i class="fab fa-github"></i></a>
          <a href="#" title="CV"><i class="fas fa-file-pdf"></i></a>
          <a href="#" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
        </div>
      </div>
      <div class="profile-image">
        <img src="images/hjl.jpg" alt="Junlin Hou">
      </div>
    </div>
  </header>

  <div class="container">
    <!-- Biography Section -->
    <section class="section">
      <h2 class="section-title">Biography</h2>
      <p>Dr. Junlin Hou is a Post-doctoral Fellow at the Department of Computer Science and Engineering, The Hong Kong University of Science and Technology (HKUST). She obtained her Ph.D. from the School of Computer Science, Fudan University in 2023. She received her B.S. degree in Mathematics and Applied Mathematics from Shanghai University in 2018. She has published 30+ papers in top-tiered conferences and journals, including CVPR, ICCV, ECCV, PR, etc. She served as a reviewer for IEEE TMI, MICCAI, ICCV, etc. She has also served as the guest editor for CMIG special issue on Trustworthy AI for Medical Imaging and the program chair for IJCAI 2024 workshop on TAI4H. She also led the team winning 10+ medical grand challenges. Her current research interest is in explainable artificial intelligence for healthcare.</p>
    </section>

    <!-- News Section -->
    <section class="section">
      <h2 class="section-title">News</h2>
      <div class="news-container">
        <h3>2025</h3>
        <div class="news-item">
          <div class="news-date">July 07</div>
          <div class="news-content">Rank 1st at <a href="https://ai-medical-image-analysis.github.io/5th/">PHAROS-AFE-AIMI Competition</a> Track 1 and Track 2 @ ICCV 2025</div>
        </div>
        <div class="news-item">
          <div class="news-date">June 18</div>
          <div class="news-content">One paper was accepted at MICCAI 2025</div>
        </div>
        <div class="news-item">
          <div class="news-date">April 15</div>
          <div class="news-content">One paper was accepted at IEEE TMI: "QMix: Quality-aware Learning with Mixed Noise for Robust Retinal Disease Diagnosis" <a href="https://arxiv.org/pdf/2404.05169">[arXiv]</a></div>
        </div>
        <div class="news-item">
          <div class="news-date">February 11</div>
          <div class="news-content">One paper was accepted at <a href="https://iclr.cc/">ICLR 2025</a></div>
        </div>

        <h3>2024</h3>
        <div class="news-item">
          <div class="news-date">October 04</div>
          <div class="news-content">A survey about self-eXplainable AI on arXiv: "<a href="https://arxiv.org/abs/2410.02331">Self-eXplainable AI for Medical Image Analysis: A Survey and New Outlooks</a>"</div>
        </div>
        <div class="news-item">
          <div class="news-date">July 22</div>
          <div class="news-content">Serve as a Guest Editor for the Special Issue <a href="https://www.sciencedirect.com/journal/computerized-medical-imaging-and-graphics/about/call-for-papers#trustworthy-artificial-intelligence-for-medical-imaging">Trustworthy Artificial Intelligence for Medical Imaging</a> for Computerized Medical Imaging and Graphics(CMIG)</div>
        </div>
        <!-- More news items... -->
      </div>
    </section>

    <!-- Research Section -->
    <section class="section">
      <h2 class="section-title">Research</h2>
      
      <div class="research-item">
        <div class="research-image">
          <img src="images/mmac1.png" alt="MMAC Research">
        </div>
        <div class="research-details">
          <div class="research-title"><a href="https://link.springer.com/chapter/10.1007/978-3-031-54857-4_3">Towards Label-Efficient Deep Learning for Myopic Maculopathy Classification</a></div>
          <div class="research-authors">Junlin Hou, Jilan Xu, Fan Xiao, Bo Zhang, Yiqian Xu, Yuejie Zhang, Haidong Zou, Rui Feng</div>
          <div class="research-conference">MICCAI 2023 Workshop. Winner of MICCAI-MMAC 2023 Challenge</div>
          <p>A label-efficient deep learning framework for myopic maculopathy classification.</p>
          <div class="research-links">
            <a href="https://github.com/FDU-VTS/MMAC"><i class="fab fa-github"></i> Code</a>
          </div>
        </div>
      </div>
      
      <div class="research-item">
        <div class="research-image">
          <img src="images/lanet.png" alt="LANet Research">
        </div>
        <div class="research-details">
          <div class="research-title"><a href="https://ieeexplore.ieee.org/abstract/document/10095713">Diabetic retinopathy grading with weakly-supervised lesion priors</a></div>
          <div class="research-authors">Junlin Hou, Fan Xiao, Jilan Xu, Rui Feng, Yuejie Zhang, Haidong Zou, Lina Lu, Wenwen Xue</div>
          <div class="research-conference">ICASSP 2023</div>
          <p>A novel weakly-supervised lesion-aware network for DR grading, which enhances the discriminative features with lesion priors by only image-level supervision.</p>
          <div class="research-links">
            <a href="https://github.com/FDU-VTS/LANet"><i class="fab fa-github"></i> Code</a>
          </div>
        </div>
      </div>
      
      <!-- More research items... -->
      
    </section>

    <!-- Awards Section -->
    <section class="section">
      <h2 class="section-title">Awards & Honors</h2>
      <ul class="custom-list">
        <li>Rank 4th at the 4th COV19D Competition, Track 1 COVID19 Detection Challenge @ CVPR 2024</li>
        <li>Winner of the 4th COV19D Competition, Track 2 COVID19 Domain Adaptation Challenge @ CVPR 2024</li>
        <li>Winner of MMAC Challenge, Track 1 Classification of Myopic Maculopathy @ MICCAI 2023</li>
        <li>Winner of MMAC Challenge, Track 2 Segmentation of Myopic Maculopathy Plus Lesions @ MICCAI 2023</li>
        <li>Shanghai Outstanding Graduate Award, 2023</li>
        <!-- More awards... -->
      </ul>
    </section>

    <!-- Education Section -->
    <section class="section">
      <h2 class="section-title">Education</h2>
      <ul class="custom-list">
        <li>Ph.D. in Computer Science, Fudan University, Shanghai, China. 2018.09-2023.07</li>
        <li>Bachelor of Science (Mathematics), Shanghai University, Shanghai, China. 2014.09-2018.07</li>
      </ul>
    </section>

    <!-- Service Section -->
    <section class="section">
      <h2 class="section-title">Service</h2>
      <p><strong>Workshop/Challenge Organizer:</strong></p>
      <ul class="custom-list">
        <li>Guest Editor, Special Issue <a href="https://www.sciencedirect.com/journal/computerized-medical-imaging-and-graphics/about/call-for-papers#trustworthy-artificial-intelligence-for-medical-imaging">TAI4MI</a> for Computerized Medical Imaging and Graphics (CMIG)</li>
        <li>Co-chair, The Second International Workshop on Trustworthy Artificial Intelligence for Healthcare (<a href="https://sites.google.com/view/tai4h2024/">TAI4H</a>) @ IJCAI 2024</li>
        <li>Co-organizer, Ultra-Widefield Fundus Imaging for Diabetic Retinopathy Challenge 2024 (<a href="https://codalab.lisn.upsaclay.fr/competitions/18605">UWF4DR</a>) @ MICCAI 2024</li>
      </ul>
      
      <p><strong>Reviewer:</strong></p>
      <ul class="custom-list">
        <li>Conference: MICCAI24, ICCV23, ICASSP23, ICIP22</li>
        <li>Journal: Neurocomputing, AI in Medicine</li>
      </ul>
      
      <p><strong>Teaching Assistant:</strong> Theory of Computation</p>
    </section>
  </div>

  <footer class="footer">
    <div class="container">
      <p>© 2023 Junlin Hou. Last updated: July 2023.</p>
      <p>Inspired by <a href="https://jonbarron.info/">Jon Barron</a></p>
    </div>
  </footer>

  <a href="#" class="back-to-top" id="backToTop"><i class="fas fa-arrow-up"></i></a>

  <script>
    // Back to top button
    const backToTopButton = document.getElementById('backToTop');
    
    window.addEventListener('scroll', () => {
      if (window.pageYOffset > 300) {
        backToTopButton.classList.add('visible');
      } else {
        backToTopButton.classList.remove('visible');
      }
    });
    
    backToTopButton.addEventListener('click', (e) => {
      e.preventDefault();
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  </script>
</body>
</html>








<!-- <!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Junlin Hou</title>
  
  <meta name="author" content="Junlin Hou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Junlin Hou</name>
		<br>
		 <a href="mailto:[deletethis] csejlhou@ust.hk", style="color:grey">csejlhou at ust dot hk</a>
		<br>
		<br>
              <p>My name is Junlin Hou (侯君临), I am a Post-doc Fellow at the Department of Computer Science and Engineering, <a href=https://hkust.edu.hk>The Hong Kong University of Science and Technology (HKUST)</a>, working with Professor <a href=https://cse.hkust.edu.hk/~jhc/>Hao Chen</a>. Before that, I obtained my PhD from the School of Computer Science, <a href=https://www.fudan.edu.cn>Fudan University</a>, supervised by Professor <a href=https://faculty.fudan.edu.cn/fengrui/zh_CN/zdylm/667351/list/index.htm>Rui Feng</a>.</p>
              <p>My research interest is in explainable aritificial intelligence for medical imaging, at an intersection of machine learning, computer vision, and medical imaging. My recent research topics include:</p>
              <li>Self-explainable (intrisically interpretable) deep learning</li>
              <li>Label-efficient deep learning</li>
              <li>Opthalmology, radiology, dermoscopy</li>
              <!-- <p> Dr. Junlin Hou is a Post-doctoral Fellow at the Department of Computer Science and Engineering, The Hong Kong University of Science and Technology (HKUST). She obtained her Ph.D. from the School of Computer Science, Fudan University in 2023, supervised by Professor Rui Feng. She received her B.E degree in Mathematics and Applied Mathematics from Shanghai University in 2018. She has published 20+ papers in top-tiered conferences and journals, including CVPR, ICCV, ECCV, PR, etc. She served as a reviewer for ICCV, IEEE TMI, AI in medicine, etc. Her current research interest is in explainable artificial intelligence for healthcare. -->
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=2bHYfQcAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/houjunlin"> GitHub </a> 
              </p>
            </td>
            <style>
              
            </style> 
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/hjl.jpg"><img style="width:70%;max-width:70%;border-radius:50%" alt="profile photo" src="images/hjl.jpg" class="img-circle" ></a>
            </td>
          </tr>
        </tbody></table>

      </div>


      <!--<div class="post on-list">-->
      <div class="post-content">
        <h2 href="/news/">News</h2>
        
        <!--overflow-x: hidden;-->
        <style>
            .summary {
                height: 367px; 
                overflow-x: hidden;
                overflow-y: scroll !important;
            }
        </style>
     
        <div class="summary">
          
        <h3 id="2025">2025</h3>
		<p>[07. 07] Rank 1st at <a href=https://ai-medical-image-analysis.github.io/5th/>PHAROS-AFE-AIMI Competition</a> Track 1 and Track 2 @ ICCV 2025 </p>
		<p>[06. 18] One paper was accepted at MICCAI 2025 </p>
        <p>[04. 15] One paper was accepted at IEEE TMI: "QMix: Quality-aware Learning with Mixed Noise for Robust Retinal Disease Diagnosis" <a href=https://arxiv.org/pdf/2404.05169> [arXiv] </a></p>
	<p>[02. 11] One paper was accepted at <a href=https://iclr.cc/>ICLR 2025</a></p>
          
        <h3 id="2024">2024</h3>
        <p>[10. 04] A survey about self-eXplainable AI on arXiv: "<a href=https://arxiv.org/abs/2410.02331> Self-eXplainable AI for Medical Image Analysis: A Survey and New Outlooks</a>"</p>
	<p>[07. 22] Serve as a Guest Editor for the Special Issue <a href=https://www.sciencedirect.com/journal/computerized-medical-imaging-and-graphics/about/call-for-papers#trustworthy-artificial-intelligence-for-medical-imaging> Trustworthy Artificial Intelligence for Medical Imaging </a> for Computerized Medical Imaging and Graphics(CMIG)</p>
        <p>[06. 18] One paper was accepted at <a href=https://conferences.miccai.org/2024/en/> MICCAI 2024</a>: "Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis" <a href=https://arxiv.org/pdf/2404.05997> [arXiv] </a></p>
        <p>[04. 01] Serve as the program committee of the IJCAI 2024 workshop on <a href=https://sites.google.com/view/tai4h2024/> TAI4H</a></p>
        <p>[03. 22] Rank 1st at <a href=https://mlearn.lincoln.ac.uk/ai-mia-cov19d-competition/> 4th-COV19D Competition</a> Track 2 and 4th at Track1 @ CVPR 2024</p>
        <p>[02. 29] Serve as a reviewer for <a href=https://conferences.miccai.org/2024/>MICCAI 2024</a></p>
        <p>[02. 27] One paper was accepted at <a href=https://cvpr.thecvf.com>CVPR 2024</a></p>
        <p>[01. 22] Serve as a reviewer for Neurocomputing</p>

        <h3 id="2023">2023</h3>
        <p>[10. 16] Rank 1st at <a href=https://codalab.lisn.upsaclay.fr/competitions/12441>Myopic Maculopathy Analysis Challenge</a> Track 1 and Track 2 @ MICCAI 2023</p>
        <p>[08. 01] I start as a Post-doc Fellow at The Hong Kong University of Science and Technology</p>
        <p>[05. 17] I successfully defended my dissertation at Fudan University and became a PhD! </p>


      </div>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Biography</heading>
              <p>
                  <ul>
                    Dr. Junlin Hou is is a Post-doctoral Fellow at the Department of Computer Science and Engineering, The Hong Kong University of Science and Technology (HKUST). She obtained her Ph.D. from the School of Computer Science, Fudan University in 2023. She received her B.S. degree in Mathematics and Applied Mathematics from Shanghai University in 2018. She has published 30+ papers in top-tiered conferences and journals, including CVPR, ICCV, ECCV, PR, etc. She served as a reviewer for IEEE TMI, MICCAI, ICCV, etc. She has also served as the guest editor for CMIG special issue on Trustworthy AI for Medical Imaging and the program chair for IJCAI 2024 workshop on TAI4H. She also led the team winning 10+ medical grand challenges. Her current research interest is in explainable artificial intelligence for healthcare.
                  </ul>
              </p>
            </td>
          </tr> 
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>

          </td>
            </tr>
        </tbody></table>
        <style>
          img {
            border-radius: 15px;
          }
        </style>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

               <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/mmac1.png" alt="mmac1" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-54857-4_3">
                    <papertitle> Towards Label-Efficient Deep Learning for Myopic Maculopathy Classification </papertitle>
                  </a>
                  <br>
			                <b>Junlin Hou</b>, Jilan Xu, Fan Xiao, Bo Zhang, Yiqian Xu, Yuejie Zhang, Haidong Zou, Rui Feng
                  <br>
                  <em>MICCAI</em> 2023 Workshop. <font color='red'>Winner of MICCAI-MMAC 2023 Challenge</font> &nbsp 
                  <br>
                  <a href="https://github.com/FDU-VTS/MMAC">code</a>
                  <p></p>
                  <p>A label-efficient deep learning framework for myopic maculopathy classification. </p>
                </td>
              </tr>		
		
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/lanet.png" alt="lanet" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10095713">
                    <papertitle> Diabetic retinopathy grading with weakly-supervised lesion priors </papertitle>
                  </a>
                  <br>
			                <b>Junlin Hou</b>, Fan Xiao, Jilan Xu, Rui Feng, Yuejie Zhang, Haidong Zou, Lina Lu, Wenwen Xue
                  <br>
                  <em>ICASSP</em> 2023 &nbsp 
                  <br>
                  <a href="https://github.com/FDU-VTS/LANet">code</a>
                  <p></p>
                  <p>A novel weakly-supervised lesion-aware network for DR grading, which enhances the discriminative features with lesion priors by only image-level supervision. </p>
                </td>
              </tr>		
			
		
	      <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ovsegmentor.png" alt="ovsegmentor" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2301.09121">
                    <papertitle> Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision </papertitle>
                  </a>
                  <br>
			                Jilan Xu, <b>Junlin Hou</b>, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, Weidi Xie
                  <br>
                  <em>CVPR</em> 2023 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2301.09121.pdf">arXiv</a> / <a href="https://jazzcharles.github.io/OVSegmentor">project page</a> / <a href="https://github.com/Jazzcharles/OVSegmentor">code</a>
                  <p></p>
                  <p>Training open-vocabulary semantic segmentation models with image-text pairs only, which enables zero-transfer to various segmentation datasets. </p>
                </td>
              </tr>
		

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cream.png" alt="cream" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2205.13922">
                    <papertitle> CREAM: Weakly supervised object localization via class re-activation mapping </papertitle>
                  </a>
                  <br>
			                Jilan Xu, <b>Junlin Hou</b>, Yuejie Zhang, Rui Feng, Rui-Wei Zhao, Tao Zhang, Xuequan Lu, Shang Gao
                  <br>
                  <em>CVPR</em> 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2205.13922.pdf">arXiv</a>
                  <p></p>
                  <p>A weakly-supervised object localization model that generates better CAMs via soft-clustering algorithms. </p>
                </td>
              </tr>

	      <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/crossfit.png" alt="crossfit" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9995459">
                    <papertitle> Cross-field transformer for diabetic retinopathy grading on two-field fundus images </papertitle>
                  </a>
                  <br>
			                <b>Junlin Hou</b>, Jilan Xu, Fan Xiao, Rui-Wei Zhao, Yuejie Zhang, Haidong Zou, Lina Lu, Wenwen Xue, Rui Feng
                  <br>
                  <em>BIBM</em> 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2211.14552">arXiv</a> / <a href="https://github.com/FDU-VTS/DRTiD">code</a>
                  <p></p>
                  <p>A new benchmark dataset (DRTiD) for twof-field DR grading. A novel DR grading approach, namely Cross-Field Transformer (CrossFiT), to capture the correspondence between two fields as well as the long-range spatial correlations within each field.  </p>
                </td>
              </tr>
		
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cmcv2.png" alt="cmcv2" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2211.14557">
                    <papertitle> CMC_v2: Towards More Accurate COVID-19 Detection with Discriminative Video Priors </papertitle>
                  </a>
                  <br>
                      <b>Junlin Hou</b>, Jilan Xu, Nan Zhang, Yi Wang, Yuejie Zhang, Xiaobo Zhang, Rui Feng
                  <br>
                  <em>ECCV</em> 2022 AIMIA Workshop. <font color='red'>Winner of ECCV-2nd COV19D challenge</font> &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2211.14557.pdf">arXiv</a> / <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution">code</a>
                  <p></p>
                  <p>A Transformer-based model with contrastive representation enhancement. Winner of the 2nd COVID-19 Detection in ECCV 2022. </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cmcv1.png" alt="cmcv1" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf">
                    <papertitle> CMC-COV19D: Contrastive Mixup Classification for COVID-19 Diagnosis </papertitle>
                  </a>
                  <br>
                      <b>Junlin Hou</b>*, Jilan Xu*, Rui Feng, Yuejie Zhang, Fei Shan, Weiya Shi
                  <br>
                  <em>ICCV</em> 2021, AIMIA Workshop. <font color='red'>Winner of ICCV-1st COV19D challenge</font>&nbsp 
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/papers/Hou_CMC-COV19D_Contrastive_Mixup_Classification_for_COVID-19_Diagnosis_ICCVW_2021_paper.pdf">paper</a> / <a href="https://github.com/houjunlin/Team-FDVTS-COVID-Solution">code</a>
                  <p></p>
                  <p>A ResNest-50 model combined with contrastive mixup technique for 3D COVID-19 CT image classification. Winner of the 1st COVID-19 detection challenge.</p>
                </td>
              </tr> 

              
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/pr.jpg" alt="pr" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.sciencedirect.com/science/article/pii/S0031320321001928">
                    <papertitle> Periphery-aware COVID-19 diagnosis with contrastive representation enhancement </papertitle>
                  </a>
                  <br>
                    <b>Junlin Hou</b>, Jilan Xu, Longquan Jiang, Shanshan Du, Rui Feng, Yuejie Zhang, Fei Shan, Xiangyang Xue
                  <br>
                  <em>Pattern Recognition</em> 2021 &nbsp 
                  <br>
                  <a href="https://github.com/FDU-VTS/Periphery-aware-COVID">code</a>
                  <p></p>
                  <p>A novel diagnosis approach with spatial pattern prior and representation enhancement mechanism is proposed to distinguish COVID-19 in the complex scenario of multi-type pneumonia classification..</p>
                </td>
              </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards & Honors</heading>
              <p>
                  <ul>
			  <li>Rank 4th at the 4th COV19D Competition, Track 1 COVID19 Detection Challenge @ CVPR 2024</li>
		          <li>Winner of the 4th COV19D Competition, Track 2 COVID19 Domain Adaptation Challenge @ CVPR 2024 </li>
		          <li>Winner of MMAC Challenge, Track 1 Classification of Myopic Maculopathy @ MICCAI 2023</li>
			  <li>Winner of MMAC Challenge, Track 2 Segmentation of Myopic Maculopathy Plus Lesions @ MICCAI 2023</li>
			  <li>Shanghai Oustanding Graduate Award, 2023</li>
                          <li>Winner of the 2nd COVID-19 Detection Challenge @ ECCV 2022</li>
                          <li>Winner of the 1st COVID-19 Severity Detection Challenge @ ECCV 2022</li>
		          <li>Rank 3rd at Diabetic Retinopathy Analysis Challenge (DRAC) @ MICCAI 2022</li>
                          <li>Winner of the 1st COVID-19 Detection Challenge @ ICCV 2021 </li>
		          <li>Second Prize (12%) of 18th China Postgraduate Mathematical Contest in Modeling, 2021</li>
		          <li>Second Prize (12%) of 15th China Postgraduate Mathematical Contest in Modeling, 2018</li>
			  <li>Shanghai Oustanding Graduate Award, 2018</li>
                  </ul>
              </p>
            </td>
          </tr> 
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
              <p>
                  <ul>
                    <li>Ph.D. in Computer Science, Fudan University, Shanghai, China. 2018.09-2023.07</li>
                    <li>Bachelor of Science (Mathematics), Shanghai University, Shanghai, China. 2014.09-2018.07</li>
<!-- 		    <li>Zhejiang Wenzhou High School, Wenzhou, Zhejiang, China. 2011.09-2014.07</li>
		    <li>Shanghai Peijia Bilingual School, Shanghai, China. 2007.09-2011.07</li>
		    <li>Experimental Primary School of Dongtou, Wenzhou, Zhejiang, China. 2002.09-2007.07</li> -->
                  </ul>
              </p>
            </td>
          </tr> 
        </tbody></table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
              <p>
                <strong> Workshop/Challenge Organizer </strong>: 
                  <ul>
			  <li>Guest Editor, Special Issue <a href=https://www.sciencedirect.com/journal/computerized-medical-imaging-and-graphics/about/call-for-papers#trustworthy-artificial-intelligence-for-medical-imaging> TAI4MI </a> for Computerized Medical Imaging and Graphics (CMIG)</li>
			  <li>Co-chair, The Second International Workshop on Trustworthy Artificial Intelligence for Healthcare (<a href=https://sites.google.com/view/tai4h2024/>TAI4H</a>) @ IJCAI 2024</li>
			  <li>Co-organizer, Ultra-Widefield Fundus Imaging for Diabetic Retinopathy Challenge 2024 (<a href=https://codalab.lisn.upsaclay.fr/competitions/18605>UWF4DR</a>) @ MICCAI 2024</li>
                  </ul>
              </p>
              <p>
                <strong> Reviewer </strong>: 
                  <ul>
			  <li>Conference: MICCAI24, ICCV23, ICASSP23, ICIP22</li>
			  <li>Journal: Neurocomputing, AI in Medicine</li>
                  </ul>		      
              </p>
              <p>
                <strong> Teaching Assistant </strong>:  Theory of Computation
              </p>
            </td>
          </tr> 
        </tbody></table>

        <div id="topbtn-md">
          <a href="#" type="button" class="btn btn-light">
              <span class="lang-en">Top</span>
          </a>
        </div>

<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Co-supervised Students</heading>
              <p>
                  <ul>
                    <li>Kexian Tang (2023 Summer intern)</li>
                    <li>Tong Wu (2024 Summer UROP)</li>
		    <li>Hanzhe Wan (2024 Summer UROP)</li>
		    <li>Liqi Lin (2024 Summer intern)</li>
		    <li>Shuting Xu (2024 Online intern)</li>
		    <li>Ziming Wang (2024 Fall UROP)</li>
                  </ul>
              </p>
            </td>
          </tr> 
        </tbody></table> -->

	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
 -->
